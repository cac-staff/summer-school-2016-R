---
layout: page
title: "Performance Optimization and Parallelization"
subtitle: Introduction to Rbenchmark and plyr/doParallel
minutes: 30
---

```{r, include=FALSE}
source("tools/chunk-options.R")
```

> ## Learning Objectives {.objectives}
>
> * Understand several common R performance bottlenecks
> * Learn how to benchmark code
> * Properly identify instances where code might benefit from parallelization 
> * Be able to parallelize R code 

Although R is a fantastic tool for data analysis and exploration, speed is not 
one of its strengths. This is by design - R was made to be easy and powerful to 
use, and takes many performance shortcuts for the sake of usability. That said, 
simply knowing several common mistakes to avoid can often reduce the execution 
times of your scripts from hours to minutes. 

Before we start, make sure you have the following packages installed:

```{r package install, eval=FALSE}
install.packages(c("microbenchmark", "plyr", "doParallel"))
```

## Benchmarking

Before we can start to cover what slows code down, we'll need a way of measuring
how fast it is. This is called benchmarking. The `microbenchmark` package 
provides a very easy way of doing this. Simply give it a piece of code to 
execute, and it will run your code repeatedly, reporting the amount of time it 
took to run. Less efficient code is slower.

As an example, let's generate 100 random numbers and see how long it takes to 
compute their square root.
```{r benchmarking}
library(microbenchmark)

randoms <- runif(100)
microbenchmark(sqrt(randoms))
```

We can also compare two pieces of code at once. Just specify two pieces of code 
to compare. In this case, we will compare the execution time of R's `sqrt()` 
function and the `^` exponent operator.

```{r comparisons}
microbenchmark(sqrt(randoms), randoms ^ 0.5)
```

As it turns out, `sqrt()` is significantly faster. Using this method, we'll be 
able to figure out what code is good (fast) and what code could use improvement.

## Common performance optimizations

Although there's not enough time to cover every possible performance 
optimization in R, we'll cover several common mistakes that have a major impact 
on code performance. 

### Preallocate for-loops

Let's cover two examples of the same piece of code: in this case, just creating 
a sequence of numbers of a specified size. For the first case (`dynamic`), we 
will grow the variable `sequence` with each iteration of our loop. In the second 
case, we will create `sequence` beforehand at its expected final size.

```{r preallocate memory}
dynamic <- function(size) {
  sequence <- NA
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

preallocated <- function(size) {
  sequence <- rep(NA, size)
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

microbenchmark(dynamic(10),
               dynamic(100),
               dynamic(1000),
               dynamic(10000),
               preallocated(10),
               preallocated(100),
               preallocated(1000),
               preallocated(10000))
```

Preallocating the `sequence` variable of our loop performed better in every case. 
Intriguingly, the speed advantage of preallocating `sequence` vs. dynamically 
growing it actually increases with larger sizes.

The reason for the difference in speed is caused by how R handles stored objects
in memory. Every time it has to make `sequence` bigger, R actually has to create
a copy of `sequence` at the new size, and then add `i` to the new object. By 
preallocating sequence at its final size, we avoid all of this unnecessary 
copying.

Note that all of the `ply` functions from the `plyr` function do this 
internally. The easiest way to avoid the pitfalls of dynamically resizing an 
object is to simply use the corresponding `*ply` function.

The equivalent `ply` function for the last example, compared to `preallocated()`:

```{r ply loop}
library(plyr)

microbenchmark(
  preallocated(10000),
  llply(1:10000, function(x) {
    return(x)
  }))
```

### Avoid creating unnecessary variables

The more things you make your computer do, the longer it will take. Saving 
variables with `<-` is not an instantaneous operation, especially if variables 
are comparatively large. In this case we will calculate the mean gdpPercap for a
particular country using the `gapminder` dataset. In our `extra_variables()` 
function, we will save all of the data for the country of interest as 
countryData, then calculate the mean `gdpPercap`. In the second case, 
we calculate `gdpPercap` in a single step.

```{r unnecessary vars}

library(gapminder)

extra_variables <- function(country) {
  countryData <- gapminder[gapminder$country == country, ]
  return(mean(countryData$gdpPercap))
}

less_variables <- function(country) {
  return(mean(gapminder$gdpPercap[gapminder$country == country]))
}

microbenchmark(extra_variables("Germany"), less_variables("Germany"))
```

Using less variables is faster. That said, keep in mind that there is a balance 
to be struck between having less variables and having readable code. If you do 
not understand what a piece of code is doing at first glance, break it into 
more readable chunks and add comments wherever possible.

### 

## Parallelization with `plyr` and `doParallel`

```{r detect cores}
library(doParallel)
detectCores()
```
