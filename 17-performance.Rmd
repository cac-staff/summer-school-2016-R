---
layout: page
title: "Performance Optimization and Parallelization"
minutes: 30
---

```{r, include=FALSE}
source("tools/chunk-options.R")
```

> ## Learning Objectives {.objectives}
>
> * Learn how to benchmark code
> * Understand several common R performance bottlenecks
> * Properly identify instances where code might benefit from parallelization 
> * Be able to parallelize R code 

Although R is a fantastic tool for data analysis and exploration, speed is not 
one of its strengths. This is by design - R was made to be easy and powerful to 
use, and takes many performance shortcuts for the sake of usability. That said, 
simply knowing several common mistakes to avoid can often reduce the execution 
times of your scripts from hours to minutes. 

Before we start, make sure you have the following packages installed:

```{r package install, eval=FALSE}
install.packages(c("microbenchmark", "plyr", "doParallel"))
```

## Benchmarking

Before we can start to cover what slows code down, we'll need a way of measuring
how fast it is. This is called benchmarking. The `microbenchmark` package 
provides a very easy way of doing this. Simply give it a piece of code to 
execute, and it will run your code repeatedly, reporting the amount of time it 
took to run. Less efficient code is slower.

As an example, let's generate 100 random numbers and see how long it takes to 
compute their square root.
```{r benchmarking}
library(microbenchmark)

randoms <- runif(100)
microbenchmark(sqrt(randoms))
```

We can also compare two pieces of code at once. Just specify two pieces of code 
to compare. In this case, we will compare the execution time of R's `sqrt()` 
function and the `^` exponent operator for taking a square root.

```{r comparisons}
microbenchmark(sqrt(randoms), randoms ^ 0.5)
```

After benchmarking it would appear that `sqrt()` is significantly faster. Using 
comparisons like this, we'll be able to figure out what code is good (fast) and 
what code could use improvement.

## Common performance optimizations

Although there's not enough time to cover every possible performance 
optimization in R, we'll cover several common mistakes that have a major impact 
on code performance. 

### Preallocate for-loops

Let's cover two examples of the same piece of code: in this case, just creating 
a sequence of numbers of a certain length. For the first case (`dynamic`), we 
will grow the variable `sequence` with each iteration of our loop. In the second 
case, we will create `sequence` beforehand at its expected final size.

```{r preallocate memory}
dynamic <- function(size) {
  sequence <- NA
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

preallocated <- function(size) {
  sequence <- rep(NA, size)
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

microbenchmark(dynamic(10),
               dynamic(100),
               dynamic(1000),
               dynamic(10000),
               preallocated(10),
               preallocated(100),
               preallocated(1000),
               preallocated(10000))
```

Preallocating the `sequence` variable of our loop performed better in every case. 
Intriguingly, the speed advantage of preallocating `sequence` vs. dynamically 
growing it actually increases with larger sizes. The reason for the difference 
in speed is caused by how R handles stored objects
in memory. Every time it has to make `sequence` bigger, R actually has to create
a copy of `sequence` at the new size, and then add `i` to the new object. By 
preallocating sequence at its final size, we avoid all of this unnecessary 
copying.

Note that all of the `*ply` functions from the `plyr` function do this 
internally. The easiest way to avoid the pitfalls of dynamically resizing an 
object is to simply use the corresponding `*ply` function.

The equivalent `*ply` function for the last example, compared to `preallocated()`:

```{r ply loop}
library(plyr)

microbenchmark(
  preallocated(10000),
  llply(1:10000, function(x) return(x)))
```

### Avoid creating unnecessary variables

The more things you make your computer do, the longer it will take. Saving 
variables with `<-` is not always an instantaneous operation, especially if the 
variables you are saving are comparatively large. In this case we will calculate
the mean gdpPercap for a particular country using the `gapminder` dataset. In 
our `extra_variables()` function, we will save all of the data for the country 
of interest as `countryData`, then calculate the mean `gdpPercap`. In the second 
case, we calculate `gdpPercap` in a single step.

```{r unnecessary vars}

library(gapminder)

extra_variables <- function(country) {
  countryData <- gapminder[gapminder$country == country, ]
  return(mean(countryData$gdpPercap))
}

less_variables <- function(country) {
  return(mean(gapminder$gdpPercap[gapminder$country == country]))
}

microbenchmark(extra_variables("Germany"), less_variables("Germany"))
```

Using less variables is faster. That said, keep in mind that there is a balance 
to be struck between having less variables and having readable code. If you do 
not understand what a piece of code is doing at first glance, it's a great idea 
to break it into more readable chunks and add comments that explain what your 
code is doing.

### Use vectorized code

Many of R's core functions use actually use ultra-fast code written in other 
programming languages like C and Fortran. One major example of this is using R's
vectorized operators and functions (that take a vector as input and return a 
vector as output). One example of this is adding a number to a vector: in the 
first case, we will simply add 10 to a set of 1000 random numbers with the `+`
operator. In the second case, we will loop over all 1000 numbers and add 10 to
each. 

```{r vectors}
randoms <- runif(1000)

microbenchmark(randoms + 10, 
               for (i in 1:1000) {
                 randoms[i] + 10
               })
```

Although the computer is in fact performing the same operation in both cases
(it loops over the vector), the vectorized operation is almost 200x faster 
because it is using an ultra-fast implementation written in another language.

### Avoid repeatedly writing or reading from disk

Using the hard disk on your computer is an extremely slow operation. Objects 
stored in memory (the R workspace) are orders of magnitude faster to access. If 
one of your scripts is reading or writing to disk repeatedly, your code will be 
limited by your disk I/O speed.

As an example of this, we will save the data from a country as a in 
memory and then read it back. We will then do the same thing, but instead of 
using memory, we will save it to disk and then read it back. 

```{r mem vs disk}

memory <- function(country) {
  data <- gapminder[gapminder$country == country, ]
  return(data)
}

disk <- function(country) {
  write.csv(gapminder[gapminder$country == country, ], 
            file = "temp.csv", row.names = FALSE)
  return(read.csv("temp.csv", as.is = TRUE))
}

microbenchmark(memory("Germany"), disk("Germany"))
file.remove("temp.csv")  # clean up after ourselves
```

## Parallelization with `plyr` and `doParallel`

One final way to have code run faster is to simply run it with more computing 
power. Keep in mind however, that faster code will *always* beat faster 
hardware. As we've shown in previous 
examples, optimizing your code can often result in it running hundreds, if not
thousands of times faster. The performance boost offered by parallelizing code 
is comparatively modest and is limited to the number of CPUs you have available.

Furthermore, some code simply cannot be parallelized. At its heart, all parallel
computing involves splitting a large operation into separate chunks, performing 
operations on separate pieces of hardware, and then aggregating the results. If 
one operation depends on the last or otherwise modifies a variable used by other 
processes, it cannot be done in parallel. 

Problems that are a good fit for parallel computing typically
meet the following criteria:

+ The code takes a long time to execute
+ Each computation is completely independent of other computations
+ The code has already been optimized as much as possible

With that said, let's write some parallel code! We will use `plyr` and the 
`doParallel` package, as those packages are easy to use and not dependent on OS.

How many cores do we have available?

```{r detect cores}
library(doParallel)
detectCores()
```

The machine this tutorial was written on has `r detectCores()` cores. Let's use 
'em. For our example calculation, we will try to calculate the 95% confidence 
interval for 

```{r single thread}
library(plyr)

bootstrap <- function(x) {
  return(summary(runif(100000)))
}
benchmark(llply(1:10000, bootstrap), replications = 1)
```

To parallelize a function using one of the `*ply` functions, it's as simple as 
registering our parallel backend and adding the argument `.parallel = TRUE`.

```{r parallel}
registerDoParallel(cores = 4)
benchmark(
  ldply(1:1000, bootstrap),
  ldply(1:1000, bootstrap, .parallel = TRUE), 
  replications = 1)
```
