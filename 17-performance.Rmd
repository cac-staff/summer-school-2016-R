---
layout: page
title: "Performance Optimization and Parallelization"
minutes: 30
---

```{r, include=FALSE}
source("tools/chunk-options.R")
```

> ## Learning Objectives {.objectives}
>
> * Learn how to benchmark code
> * Understand several common R performance bottlenecks
> * Properly identify instances where code might benefit from parallelization 
> * Be able to parallelize R code 

Although R is a fantastic tool for data analysis and exploration, speed is not 
one of its strengths. This is by design - R was made to be easy and powerful to 
use, and takes many performance shortcuts for the sake of usability. That said, 
simply knowing several common mistakes to avoid can often reduce the execution 
times of your scripts from hours to minutes. 

Before we start, make sure you have the following packages installed:

```{r package install, eval=FALSE}
install.packages(c("microbenchmark", "plyr", "doParallel"))
```

## Benchmarking

Before we can start to cover what slows code down, we'll need a way of measuring
how fast it is. This is called benchmarking. The `microbenchmark` package 
provides a very easy way of doing this. Simply give it a piece of code to 
execute, and it will run your code repeatedly, reporting the amount of time it 
took to run. Less efficient code is slower.

As an example, let's generate 100 random numbers and see how long it takes to 
compute their square root.
```{r benchmarking}
library(microbenchmark)

randoms <- runif(100)
microbenchmark(sqrt(randoms))
```

We can also compare two pieces of code at once. Just specify two pieces of code 
to compare. In this case, we will compare the execution time of R's `sqrt()` 
function and the `^` exponent operator for taking a square root.

```{r comparisons}
microbenchmark(sqrt(randoms), randoms ^ 0.5)
```

After benchmarking it would appear that `sqrt()` is significantly faster. Using 
comparisons like this, we'll be able to figure out what code is good (fast) and 
what code could use improvement.

## Common performance optimizations

Although there's not enough time to cover every possible performance 
optimization in R, we'll cover several common mistakes that have a major impact 
on code performance. 

### Preallocate for-loops

Let's cover two examples of the same piece of code: in this case, just creating 
a sequence of numbers of a certain length. For the first case (`dynamic`), we 
will grow the variable `sequence` with each iteration of our loop. In the second 
case, we will create `sequence` beforehand at its expected final size.

```{r preallocate memory}
dynamic <- function(size) {
  sequence <- NA
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

preallocated <- function(size) {
  sequence <- rep(NA, size)
  for (i in 1:size) {
    sequence[i] <- i
  }
  return(sequence)
}

microbenchmark(dynamic(10),
               dynamic(100),
               dynamic(1000),
               dynamic(10000),
               preallocated(10),
               preallocated(100),
               preallocated(1000),
               preallocated(10000))
```

Preallocating the `sequence` variable of our loop performed better in every case. 
Intriguingly, the speed advantage of preallocating `sequence` vs. dynamically 
growing it actually increases with larger sizes. The reason for the difference 
in speed is caused by how R handles stored objects
in memory. Every time it has to make `sequence` bigger, R actually has to create
a copy of `sequence` at the new size, and then add `i` to the new object. By 
preallocating sequence at its final size, we avoid all of this unnecessary 
copying.

Note that all of the `*ply` functions from the `plyr` function do this 
internally. The easiest way to avoid the pitfalls of dynamically resizing an 
object is to simply use the corresponding `*ply` function.

The equivalent `*ply` function for the last example, compared to `preallocated()`:

```{r ply loop}
library(plyr)

microbenchmark(
  preallocated(10000),
  llply(1:10000, function(x) return(x)))
```

### Avoid creating unnecessary variables

The more things you make your computer do, the longer it will take. Saving 
variables with `<-` is not always an instantaneous operation, especially if the 
variables you are saving are comparatively large. In this case we will calculate
the mean gdpPercap for a particular country using the `gapminder` dataset. In 
our `extra_variables()` function, we will save all of the data for the country 
of interest as `countryData`, then calculate the mean `gdpPercap`. In the second 
case, we calculate `gdpPercap` in a single step.

```{r unnecessary vars}

library(gapminder)

extra_variables <- function(country) {
  countryData <- gapminder[gapminder$country == country, ]
  return(mean(countryData$gdpPercap))
}

less_variables <- function(country) {
  return(mean(gapminder$gdpPercap[gapminder$country == country]))
}

microbenchmark(extra_variables("Germany"), less_variables("Germany"))
```

Using less variables is faster. That said, keep in mind that there is a balance 
to be struck between having less variables and having readable code. If you do 
not understand what a piece of code is doing at first glance, it's a great idea 
to break it into more readable chunks and add comments that explain what your 
code is doing.

### Use vectorized code

Many of R's core functions use actually use ultra-fast code written in other 
programming languages like C and Fortran. One major example of this is using R's
vectorized operators and functions (that take a vector as input and return a 
vector as output). One example of this is adding a number to a vector: in the 
first case, we will simply add 10 to a set of 1000 random numbers with the `+`
operator. In the second case, we will loop over all 1000 numbers and add 10 to
each. 

```{r vectors}
randoms <- runif(1000)

microbenchmark(randoms + 10, 
               for (i in 1:1000) {
                 randoms[i] + 10
               })
```

Although the computer is in fact performing the same operation in both cases
(it loops over the vector), the vectorized operation is almost 200x faster 
because it is using an ultra-fast implementation written in another language.

### Avoid repeatedly writing or reading from disk

Using the hard disk on your computer is an extremely slow operation. Objects 
stored in memory (the R workspace) are orders of magnitude faster to access. If 
one of your scripts is reading or writing to disk repeatedly, your code will be 
limited by your disk I/O speed.

As an example of this, we will save the data from a country as a in 
memory and then read it back. We will then do the same thing, but instead of 
using memory, we will save it to disk and then read it back. 

```{r mem vs disk}

memory <- function(country) {
  data <- gapminder[gapminder$country == country, ]
  return(data)
}

disk <- function(country) {
  write.csv(gapminder[gapminder$country == country, ], 
            file = "temp.csv", row.names = FALSE)
  return(read.csv("temp.csv", as.is = TRUE))
}

microbenchmark(memory("Germany"), disk("Germany"))
file.remove("temp.csv")  # clean up after ourselves
```


## Parallelization with `plyr` and `doParallel`

One final way to have code run faster is to simply run it with more computing 
power. Keep in mind however, that faster code will *always* beat faster 
hardware. As we've shown in previous 
examples, optimizing your code can often result in it running hundreds, if not
thousands of times faster. The performance boost offered by parallelizing code 
is comparatively modest and is limited by the number of CPUs you have available.

Furthermore, some code simply cannot be parallelized. At its heart, all parallel
computing involves splitting a large operation into separate chunks, performing 
operations on separate pieces of hardware, and then aggregating the results. If 
one operation depends on the last or otherwise modifies a variable used by other 
processes, it cannot be done in parallel. 

Put more simply, problems that are a good fit for parallel computing typically
meet the following criteria:

+ The code takes a long time to execute
+ Each computation is completely independent of other computations
+ The code has already been optimized as much as possible

With that said, let's write some parallel code! We will use `plyr` and the 
`doParallel` package, as those packages are easy to use and can be run on any OS
(OSX/Linux/Windows).

How many cores do we have available?

```{r detect cores}
library(doParallel)
detectCores()
```

The machine this tutorial was written on has `r detectCores()` cores. Let's use 
'em. For our example calculation, we will actually cheat a bit and make function
that just returns the mean gdp for a country / 1000 (from `gapminder`) after 
waiting for a fraction of a second. The process for parallelization on is 
slightly different between UNIX (OSX/Linux) and Windows.


### On OSX/Linux

To parallelize a function using one of the `*ply` functions, it's as simple as 
registering a parallel backend with `registerDoParallel(cores)` once in your 
script and adding the argument `.parallel = TRUE` to the `*ply` function you 
want to use. That's it - you're done!

So the parallel workflow looks like this:

+ `registerDoParallel(cores)`
+ Parallel calculations with `plyr`

A sample parallel script on OSX/Linux might look like this:
```{r parallel}
library(plyr)
library(microbenchmark)
library(gapminder)
library(doParallel)

registerDoParallel(cores = detectCores())

someVariable <- 1000

fakefunc <- function(country) {
  Sys.sleep(0.2)
  df <- gapminder[gapminder$country == country, ]
  toReturn <- data.frame(country, gdp = mean(df$gdpPercap) / someVariable)
  return(toReturn)
}

microbenchmark(
  laply(unique(gapminder$country), fakefunc),
  laply(unique(gapminder$country), fakefunc, .parallel = TRUE),
  times = 1)
```


### On Windows

Parallelizing a function on Windows is slightly harder than on OSX/Linux. 
On OSX/Linux, the workspace is copied to all of the parallel processes you use. 
On Windows, the parallel R processes must be manually created and variables and 
functions must be passed to your parallel R workers (this is done with the 
`.paropts` argument to `*ply`. Finally, once you are done using R in parallel 
(like at the end of your script), you must stop your parallel R workers with
`stopImplicitCluster()` or they will hang around and continue to eat up system
resources.

On Windows, a sample parallel workflow looks like this:

+ `registerDoParallel(cores)`
+ Parallel calculations with `plyr` (must use `.paropts` to pass variables and 
packages). `.paropts` typically looks like `.paropts = list(.packages = c("package1", "package2"), .export = c("variableName1", "functionName2"))`
+ `stopImplicitCluster()`

```{r windows parallel, eval=FALSE}
library(microbenchmark)
library(plyr)
library(doParallel)
library(gapminder)

# start parallel workers
registerDoParallel(cores = detectCores())

someVariable <- 1000

fakefunc <- function(country) {
  Sys.sleep(0.2)
  df <- gapminder[gapminder$country == country, ]
  toReturn <- data.frame(country, gdp = mean(df$gdpPercap) / someVariable)
  return(toReturn)
}

microbenchmark(
  ldply(unique(gapminder$country), fakefunc),
  ldply(unique(gapminder$country), fakefunc, .parallel = TRUE, 
        .paropts = list(.packages = "gapminder", .export = c("someVariable"))),
  times = 1)

# kill parallel workers
stopImplicitCluster()
```

### Final notes

Sometimes, if what you're 
parallelizing is very fast, your code make actually run *slower* when in parallel.
Every parallel iteration of your `*ply` function involves splitting up your job
and sending it to the different parallel R workers. If individual iterations run
slower in parallel, it is likely that your function is spending more time 
splitting up the job to run it on multiple cores than it is doing actual work. 
These types of jobs should not be done in parallel.

Attempting to `registerDoParallel(cores)` with more cores than your machine 
actually has will slow down your job. If you tried to do a 16 core computation on
a machine that only has 4 cores, for instance, the 16 R workers will be forced to
take turns on your available CPUs, resulting in them running at roughly 25% speed. 
For the same reason, do not put a parallel function inside another parallel 
function. Something like this: `**ply(data, function(x) {**ply(data, fun, .parallel = TRUE)}), .parallel = TRUE)` will 
only end badly. 

Finally when running a parallel R job on a compute cluster, **NEVER** do this: 
`registerDoParallel(cores = detectCores())`. This will attempt to use every 
available CPU (including ones you have not been allocated), potentially ruining 
other jobs on a compute node. You must register a fixed number of cores 
(`registerDoParallel(cores = 4)` for instance), and then request that number of 
cores from the scheduler.

